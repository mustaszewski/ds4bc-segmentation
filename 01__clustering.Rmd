---
title: "01_clustering"
author: "EmanuelGregorMichaelPeter"
date: "10 12 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include=FALSE}
# Here, we render the exploration file to load all necessary variables into our environment
# This will take a minute (all plots etc are created again)
rmarkdown::render("00__exploration.Rmd")
```

## Clustering

For the clustering (no matter whether optics, kmeans, hiearchical), we need to define a strategy to deal with missing values. We decided to take two approaches: (a) drop all records containing missing values, (b) drop the variables containing many missing values (i.e. age-related variables). Then we can compare both results.

However, some variables will have to be excluded anyway, because they were either used to extract other features or are per se not useful for our purpose. **This should definitely be adapted**
```{r}
variables_to_exclude = c("ID", # no useful information
                        "Postcode", # to many attributes 
                        "MERCHANDISE2015", # merchandise_any might be enough because it is a rare category anyway
                        "MERCHANDISE2016",
                        "MERCHANDISE2017",
                        "MERCHANDISE2018",
                        "MERCHANDISE2019",
                        "COUNT2015", # expressed in COUNTtotal and COUNTaverage
                        "COUNT2016",
                        "COUNT2017",
                        "COUNT2018",
                        "COUNT2019", 
                        "SUM2015", # expressed in SUMtotal and SUMaverage
                        "SUM2016",
                        "SUM2017",
                        "SUM2018",
                        "SUM2019",
                        "LastPaymentDate", # expressed in days_since_last_payment,
                        "LastPaymentYEAR",
                        "Ort", # same as postal code
                        "year_born", # expressed in age_at_last_donation
                        "LastPaymentMONTH", # expressed in days_since_last_payment
                        "PenultimatePaymentMONTH", # expressed in donation_interval
                        "PenultimatePaymentYEAR", # expressed in donation_interval
                        "age_at_last_donation" # expressed in generation_monikert
                        )
```

Check remaining features: 
```{r}
df_without_variables_to_exlude <- customer_segmentation_first_prepro %>%
  select(-all_of(variables_to_exclude))
df_without_variables_to_exlude %>% colnames()
```

```{r}
df_without_variables_to_exlude %>% skimr::skim()
```



```{r}
# approach a)
df_without_na <- df_without_variables_to_exlude %>% drop_na()
df_without_na %>% nrow() # 242480 cases

# approach b)
df_with_fewer_vars <- df_without_variables_to_exlude %>% 
  select(-c(generation_moniker, donation_interval)) %>% 
  drop_na()
df_with_fewer_vars %>% nrow() # 396694 cases
```

Also, it will be necessary to dummy-code nominal variables, and to scale numeric variables. A code-snippet for these preprocessing operations implemented in the `tidymodels` package is provided below:

```{r}
library(tidymodels)
df_without_na_prep <- df_without_na %>% recipe() %>% 
  step_scale(all_numeric()) %>%
  step_dummy(all_nominal()) %>%
  prep() %>% 
  bake(new_data = NULL)

df_with_fewer_vars_prep <- df_with_fewer_vars %>% recipe() %>% 
  step_scale(all_numeric()) %>%  
  step_dummy(all_nominal()) %>%
  prep() %>% 
  bake(new_data = NULL)

```

```{r}
df_without_na_prep
```


### kmeans

```{r eval=FALSE, include=FALSE}
set.seed(123)


gc() # garbage collection to improve performance

max_k = 20

kmeans_clust <- 
  tibble(k = 1:max_k) %>%
  mutate(
    kclust = map(k, ~kmeans(df_without_na_prep, .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, df_without_na_prep)
  )

'
produces this error:
Warning: Problem with `mutate()` column `kclust`.
i `kclust = map(k, ~kmeans(df_without_na_prep, .x))`.
i Quick-TRANSfer stage steps exceeded maximum (= 12124150)

https://stackoverflow.com/questions/21382681/kmeans-quick-transfer-stage-steps-exceeded-maximum'

clusterings <- 
  kmeans_clust %>%
  unnest(cols = c(glanced))

# look at total within sum of squares 

ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line() +
  geom_point()



```


Because of the error given above, we reduce the amount of different values for k 


```{r}
set.seed(123)


gc() # garbage collection to improve performance

max_k = 16

kmeans_clust <- 
  tibble(k = 1:max_k) %>%
  mutate(
    kclust = map(k, ~kmeans(df_without_na_prep, .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, df_without_na_prep)
  )

clusterings <- 
  kmeans_clust %>%
  unnest(cols = c(glanced))

# look at total within sum of squares 

ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line() +
  geom_point()



```

According to the elbow method, the above plot suggests that `k=5` is the optimal $k$. Using this information, we can cluster the data using `k=5`.

```{r}
kclust_k5 <- kmeans(df_without_na_prep, centers = 5)

#TODO: Assign clusters to raw data
```

### Visualise Clusters using PCA

```{r}
# compute PCA
library(factoextra)
pca_res <- prcomp(df_without_na_prep, scale = TRUE)
```

```{r}
# visualise PCA: data points are shown in 2D, color corresponds to cluster assignment
fviz_pca_ind(pca_res,
             axes = c(1,2), # show PCA dimensions 1 and 2
             geom.ind = c("point"), # surpress text labels with observation IDs
             title="K-Means Clustering (k=5) of Data Visualised in 2D Using PCA",
             habillage = kclust_k5$cluster, # color data points by k-means cluster
             addEllipses = TRUE) + # add concentration ellipses
  scale_color_brewer(palette = "Set1") # change color palette for readability
```

Let's check how much varaince is explained by the PCs: We see that 18 (out of 24 variables) dimensions are needed to explain ~ 95% of the variance. The first three PCs explain 28% of the variance, which is not overwhelming. 

```{r}
# compute variance explained by each principal component
get_eigenvalue(pca_res)  %>% round(1)
```
The bar plot below shows the above table in visual form, so it adds no further insights.

```{r}
# plot proportion of variance explained by each of first 15 principal components
fviz_eig(pca_res,
         choice = "variance",
         ncp=20,
         addlabels = T)
```
To understand which variables are associated with the PCA dimensions, we inspect the variable correlation circle. 


```{r}
fviz_pca_var(pca_res,
             axes = c(1,2),
             repel = F) # set to TRUE to avoid overlap (not all labels are shown)
```

Overlapping text makes it hard to intepret the plot; the only thing we can see clearly is that PCA dimension 2 represents gender. Thus, data points that appear in the upper part of the PCA individuals plot above are female, the ones in the lower part are male. This explains the three (or six) visually obvious clusters in the PCA plot.

To better understand the association between individual variables and each PC, we can inspect a correlation plot that shows the quality of representation of each variable (`cos2`) for each dimension. High cos2 values indicate a good representation on the variable on the given PC (= variable is close to correlation circle). The plot below confirms that dimension 2 represents gender. Dimension 3 is determined by total donation sum, Dimension 4 by christmas donors.

```{r}
library(corrplot)
var <- get_pca_var(pca_res)
corrplot(var$cos2, is.corr=FALSE)

```

Dimension 1 is represented by the variables `COUNTtotal`, `COUNTaverage`, `num_od_fonation_years`, and, to a lesser extent, `donation_interval` and `days_since_last_payment`. The variable correlation circle of the first dimension only shown below helps us understand Dimension 1 in the PCA individuals plot: Towards the right of the PCA we have donors with high total counts of donations and many donation years ("Stammkunden"?), towards the left we have donors with higher values of `donation_interval`. **TODO: What does this tell us?**


```{r}
fviz_pca_var(pca_res,
             axes = c(1,1),
             repel = T)
```






