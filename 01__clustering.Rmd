---
title: "01_clustering"
author: "EmanuelGregorMichaelPeter"
date: "10 12 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include=FALSE}
# Here, we render the exploration file to load all necessary variables into our environment
# This will take a minute (all plots etc are created again)
rmarkdown::render("00__exploration.Rmd")
```

## Clustering

For the clustering (no matter whether optics, kmeans, hiearchical), we need to define a strategy to deal with missing values. We decided to take two approaches: (a) drop all records containing missing values, (b) drop the variables containing many missing values (i.e. age-related variables). Then we can compare both results.

However, some variables will have to be excluded anyway, because they were either used to extract other features or are per se not useful for our purpose. **This should definitely be adapted**
```{r}
variables_to_exclude = c("ID", # no useful information
                        "Postcode", # to many attributes 
                        "MERCHANDISE2015", # merchandise_any might be enough because it is a rare category anyway
                        "MERCHANDISE2016",
                        "MERCHANDISE2017",
                        "MERCHANDISE2018",
                        "MERCHANDISE2019",
                        "COUNT2015", # expressed in COUNTtotal and COUNTaverage
                        "COUNT2016",
                        "COUNT2017",
                        "COUNT2018",
                        "COUNT2019", 
                        "SUM2015", # expressed in SUMtotal and SUMaverage
                        "SUM2016",
                        "SUM2017",
                        "SUM2018",
                        "SUM2019",
                        "LastPaymentDate", # expressed in days_since_last_payment,
                        "LastPaymentYEAR",
                        "Ort", # same as postal code
                        "year_born", # expressed in age_at_last_donation
                        "LastPaymentMONTH", # expressed in days_since_last_payment
                        "PenultimatePaymentMONTH", # expressed in donation_interval
                        "PenultimatePaymentYEAR", # expressed in donation_interval
                        "age_at_last_donation" # expressed in generation_monikert
                        )
```

Check remaining features: 
```{r}
df_without_variables_to_exlude <- customer_segmentation_first_prepro %>%
  select(-all_of(variables_to_exclude))
df_without_variables_to_exlude %>% colnames()
```

```{r}
df_without_variables_to_exlude %>% skimr::skim()
```



```{r}
# approach a)
df_without_na <- df_without_variables_to_exlude %>% drop_na()
df_without_na %>% nrow() # 242480 cases

# approach b)
df_with_fewer_vars <- df_without_variables_to_exlude %>% 
  select(-c(generation_moniker, donation_interval)) %>% 
  drop_na()
df_with_fewer_vars %>% nrow() # 396694 cases
```

Also, it will be necessary to dummy-code nominal variables, and to scale numeric variables. A code-snippet for these preprocessing operations implemented in the `tidymodels` package is provided below:

```{r}
library(tidymodels)
df_without_na_prep <- df_without_na %>% recipe() %>% 
  step_scale(all_numeric()) %>%
  step_dummy(all_nominal()) %>%
  prep() %>% 
  bake(new_data = NULL)

df_with_fewer_vars_prep <- df_with_fewer_vars %>% recipe() %>% 
  step_scale(all_numeric()) %>%  
  step_dummy(all_nominal()) %>%
  prep() %>% 
  bake(new_data = NULL)

```

```{r}
df_without_na_prep
```


### kmeans

```{r eval=FALSE, include=FALSE}
set.seed(123)


gc() # garbage collection to improve performance

max_k = 20

kmeans_clust <- 
  tibble(k = 1:max_k) %>%
  mutate(
    kclust = map(k, ~kmeans(df_without_na_prep, .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, df_without_na_prep)
  )

'
produces this error:
Warning: Problem with `mutate()` column `kclust`.
i `kclust = map(k, ~kmeans(df_without_na_prep, .x))`.
i Quick-TRANSfer stage steps exceeded maximum (= 12124150)

https://stackoverflow.com/questions/21382681/kmeans-quick-transfer-stage-steps-exceeded-maximum'

clusterings <- 
  kmeans_clust %>%
  unnest(cols = c(glanced))

# look at total within sum of squares 

ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line() +
  geom_point()



```


Because of the error given above, we reduce the amount of different values for k 


```{r}
set.seed(123)


gc() # garbage collection to improve performance

max_k = 16

kmeans_clust <- 
  tibble(k = 1:max_k) %>%
  mutate(
    kclust = map(k, ~kmeans(df_without_na_prep, .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, df_without_na_prep)
  )

clusterings <- 
  kmeans_clust %>%
  unnest(cols = c(glanced))

# look at total within sum of squares 

ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line() +
  geom_point()



```

According to the elbow method, the above plot suggests that `k=5` is the optimal $k$. Using this information, we can cluster the data using `k=5`.

```{r}
kclust <- kmeans(df_with_fewer_vars_prep, centers = 5)
```

### Visualise clusters using PCA

```{r}
library(factoextra)
pca_res <- prcomp(df_with_fewer_vars_prep, scale = TRUE)
```


```{r}
# visualise PCA
fviz_pca_ind(pca_res,
             axes = c(1,2),
             geom.ind = c("point"),
             title="K-Means Clustering of Data Visualised in 2D Using PCA",
             habillage = kclust$cluster, # color data points by k-means cluster
             addEllipses = TRUE) + # add concentration ellipses
  scale_color_brewer(palette = "Set1") # change color palette for readability
```


```{r}
# compute variance explained by each principal component
get_eigenvalue(pca_res)  %>% round(1)
```


```{r}
# plot proportion of variance explained by each of first 15 principal components
fviz_eig(pca_res,
         choice = "variance",
         ncp=20,
         addlabels = T)
```


